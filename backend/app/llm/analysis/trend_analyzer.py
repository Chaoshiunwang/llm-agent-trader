#!/usr/bin/env python3
"""
Enhanced Trend Analyzer - Independent LLM Optimization
獨立增強趨勢分析器，整合LLM優化概念
"""

import os
import sys
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

# Add the app directory to the path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), "..", "..", ".."))

from app.utils.stock_data import StockService


# Independent data structures (backward compatible)
@dataclass
class TrendAnalysisResult:
    """Independent trend analysis result - compatible with original"""

    symbol: str
    analysis_date: str
    timeframes: Dict[str, "TimeframeTrendResult"]
    dominant_trend: str
    complexity_score: float
    trend_consistency: float
    detected_phases: List[Dict[str, Any]]
    overall_assessment: str


@dataclass
class TimeframeTrendResult:
    """Individual timeframe trend analysis result"""

    timeframe: str
    trend_strength: float  # -1 to 1
    trend_direction: str  # 'uptrend', 'downtrend', 'sideways'
    trend_label: str
    confidence: float  # 0.0 to 1.0
    volatility: float
    key_levels: Dict[str, float]


@dataclass
class EnhancedTrendResult:
    """Enhanced trend analysis result with LLM optimization"""

    # Original trend analysis
    original_result: TrendAnalysisResult

    # LLM optimized features
    market_phase: str  # 'uptrend', 'downtrend', 'consolidation'
    trend_consistency: float  # 0-1, multi-timeframe consistency
    reversal_probability: float  # 0-1, probability of trend reversal
    momentum_status: str  # 'bullish', 'bearish', 'neutral'
    risk_level: str  # 'low', 'medium', 'high'

    # Decision context for LLM
    key_observations: List[str]
    llm_recommendations: List[str]

    # Analysis metadata
    analysis_date: str
    price: float

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            "analysis_date": self.analysis_date,
            "price": self.price,
            "market_phase": self.market_phase,
            "trend_consistency": self.trend_consistency,
            "reversal_probability": self.reversal_probability,
            "momentum_status": self.momentum_status,
            "risk_level": self.risk_level,
            "key_observations": self.key_observations,
            "llm_recommendations": self.llm_recommendations,
            "original_analysis": {
                "dominant_trend": self.original_result.dominant_trend,
                "trend_consistency": self.original_result.trend_consistency,
                "complexity_score": self.original_result.complexity_score,
                "timeframes": {
                    timeframe: {
                        "trend_direction": result.trend_direction,
                        "trend_strength": result.trend_strength,
                        "confidence": result.confidence,
                    }
                    for timeframe, result in self.original_result.timeframes.items()
                },
            },
        }


class EnhancedTrendAnalyzer:
    """
    Enhanced Trend Analyzer with LLM optimization features
    整合LLM優化概念，提供更適合實時決策的趨勢分析
    """

    def __init__(self):
        self.stock_service = StockService()

        # Core analysis parameters
        self.trend_threshold = 0.15  # Lower threshold for sensitive trend detection
        self.volatility_threshold_base = 0.025

        # Multi-timeframe windows for comprehensive analysis
        self.timeframe_windows = {
            "short": 5,  # Short-term: 5 days
            "medium_short": 10,  # Medium-short: 10 days
            "medium": 20,  # Medium-term: 20 days
        }

        # LLM optimization parameters
        self.sliding_windows = [
            5,
            10,
            20,
        ]  # Multi-timeframe analysis (removed 40-day lag)
        self.momentum_period = 10
        self.reversal_threshold = 0.3

        # Trend strength levels
        self.trend_strength_levels = {
            "very_strong": 0.7,
            "strong": 0.5,
            "moderate": 0.3,
            "weak": 0.15,
            "very_weak": 0.05,
        }

    def analyze_with_llm_optimization(
        self, symbol: str, target_date: Optional[str] = None
    ) -> EnhancedTrendResult:
        """
        Perform enhanced trend analysis with LLM optimization
        執行增強趨勢分析，包含LLM決策優化功能

        Args:
            symbol: Stock symbol (e.g., '2330.TW')
            target_date: Target analysis date (format: 'YYYY-MM-DD'), None for latest

        Returns:
            EnhancedTrendResult with both original and LLM-optimized analysis
        """

        # Get market data
        data = self._get_analysis_data(symbol, target_date)
        if not data:
            raise ValueError(f"Insufficient data for {symbol}")

        # Convert to DataFrame for analysis
        df = pd.DataFrame(data)
        df["date"] = pd.to_datetime(df["date"])
        df = df.sort_values("date").reset_index(drop=True)

        # Get current price and date
        current_price = df.iloc[-1]["close"]
        current_date = df.iloc[-1]["date"].strftime("%Y-%m-%d")

        # Perform independent multi-timeframe trend analysis
        original_result = self._analyze_market_phases_independent(data, symbol)

        # Perform LLM optimization analysis
        llm_features = self._perform_llm_optimization(df)

        # Generate decision context
        decision_context = self._generate_decision_context(
            original_result, llm_features, current_price
        )

        return EnhancedTrendResult(
            original_result=original_result,
            market_phase=llm_features["market_phase"],
            trend_consistency=llm_features["trend_consistency"],
            reversal_probability=llm_features["reversal_probability"],
            momentum_status=llm_features["momentum_status"],
            risk_level=llm_features["risk_level"],
            key_observations=decision_context["observations"],
            llm_recommendations=decision_context["recommendations"],
            analysis_date=current_date,
            price=current_price,
        )

    def _get_analysis_data(
        self, symbol: str, target_date: Optional[str] = None
    ) -> List[Dict]:
        """Get market data for analysis"""
        if target_date:
            # Get data up to target date with sufficient history
            end_date = datetime.strptime(target_date, "%Y-%m-%d")

            # Get extended data (2 years) to ensure sufficient history for filtering
            all_data = self.stock_service.get_market_data(symbol, "2y")
            if not all_data:
                return []

            # Filter data up to target date
            filtered_data = []
            for record in all_data:
                record_date = datetime.strptime(record["date"], "%Y-%m-%d")
                if record_date <= end_date:
                    filtered_data.append(record)

            # Return last 120 days for analysis (keeping original logic)
            return filtered_data[-120:] if len(filtered_data) >= 60 else filtered_data
        else:
            # Get latest data
            return self.stock_service.get_market_data(symbol, "6mo")

    def _analyze_market_phases_independent(
        self, market_data: List[Dict[str, Any]], symbol: str = "unknown"
    ) -> TrendAnalysisResult:
        """
        Independent multi-timeframe market analysis
        獨立多時間框架市場分析，替代對 AdvancedTrendAnalyzer 的依賴
        """

        if not market_data or len(market_data) < 20:
            return self._create_insufficient_data_result(symbol)

        # Prepare price data
        df = pd.DataFrame(market_data)
        df["date"] = pd.to_datetime(df["date"])
        df = df.sort_values("date").reset_index(drop=True)
        prices = df["close"].values
        dates = df["date"].tolist()

        # Analyze each timeframe
        timeframe_results = {}

        for name, window in self.timeframe_windows.items():
            if len(prices) >= window:
                result = self._analyze_single_timeframe(prices, dates, window, name)
                timeframe_results[name] = result

        # Determine dominant trend and consistency
        dominant_trend = self._determine_dominant_trend(timeframe_results)
        trend_consistency = self._calculate_overall_consistency(timeframe_results)
        complexity_score = self._calculate_complexity_score(timeframe_results)

        # Detect market phases
        detected_phases = self._detect_market_phases(df)

        # Generate overall assessment
        overall_assessment = self._generate_overall_assessment(
            dominant_trend, trend_consistency, complexity_score
        )

        return TrendAnalysisResult(
            symbol=symbol,
            analysis_date=dates[-1].strftime("%Y-%m-%d") if dates else "",
            timeframes=timeframe_results,
            dominant_trend=dominant_trend,
            complexity_score=complexity_score,
            trend_consistency=trend_consistency,
            detected_phases=detected_phases,
            overall_assessment=overall_assessment,
        )

    def _analyze_single_timeframe(
        self, prices: np.ndarray, dates: List, window: int, timeframe_name: str
    ) -> TimeframeTrendResult:
        """Analyze trend for a single timeframe window"""

        if len(prices) < window:
            return self._create_neutral_timeframe_result(timeframe_name)

        # Use recent data for analysis
        recent_prices = prices[-window:]

        # Calculate trend using linear regression
        x = np.arange(len(recent_prices))
        slope, _ = np.polyfit(x, recent_prices, 1)
        correlation = (
            np.corrcoef(x, recent_prices)[0, 1] if len(recent_prices) > 1 else 0
        )

        # Normalize slope for comparison
        normalized_slope = slope / np.mean(recent_prices)

        # Determine trend direction and strength
        trend_strength = normalized_slope
        confidence = abs(correlation)

        if abs(trend_strength) < self.trend_threshold:
            trend_direction = "sideways"
            trend_label = "横盘整理"
        elif trend_strength > 0:
            trend_direction = "uptrend"
            if trend_strength > self.trend_strength_levels["strong"]:
                trend_label = "强势上涨"
            else:
                trend_label = "温和上涨"
        else:
            trend_direction = "downtrend"
            if abs(trend_strength) > self.trend_strength_levels["strong"]:
                trend_label = "强势下跌"
            else:
                trend_label = "温和下跌"

        # Calculate volatility
        volatility = np.std(recent_prices) / np.mean(recent_prices)

        # Identify key levels (simplified)
        key_levels = {
            "support": float(np.min(recent_prices)),
            "resistance": float(np.max(recent_prices)),
            "current": float(recent_prices[-1]),
        }

        return TimeframeTrendResult(
            timeframe=timeframe_name,
            trend_strength=float(trend_strength),
            trend_direction=trend_direction,
            trend_label=trend_label,
            confidence=float(confidence),
            volatility=float(volatility),
            key_levels=key_levels,
        )

    def _create_insufficient_data_result(self, symbol: str) -> TrendAnalysisResult:
        """Create result for insufficient data case"""
        return TrendAnalysisResult(
            symbol=symbol,
            analysis_date=datetime.now().strftime("%Y-%m-%d"),
            timeframes={},
            dominant_trend="insufficient_data",
            complexity_score=0.0,
            trend_consistency=0.0,
            detected_phases=[],
            overall_assessment="數據不足，無法進行分析",
        )

    def _create_neutral_timeframe_result(
        self, timeframe_name: str
    ) -> TimeframeTrendResult:
        """Create neutral result for insufficient timeframe data"""
        return TimeframeTrendResult(
            timeframe=timeframe_name,
            trend_strength=0.0,
            trend_direction="sideways",
            trend_label="數據不足",
            confidence=0.0,
            volatility=0.0,
            key_levels={"support": 0.0, "resistance": 0.0, "current": 0.0},
        )

    def _determine_dominant_trend(self, timeframe_results: Dict) -> str:
        """Determine the dominant trend across all timeframes"""
        if not timeframe_results:
            return "insufficient_data"

        uptrend_weight = 0
        downtrend_weight = 0

        for result in timeframe_results.values():
            weight = result.confidence  # Use confidence as weight

            if result.trend_direction == "uptrend":
                uptrend_weight += weight
            elif result.trend_direction == "downtrend":
                downtrend_weight += weight

        total_weight = uptrend_weight + downtrend_weight

        if total_weight == 0:
            return "sideways"
        elif uptrend_weight > downtrend_weight * 1.2:  # 20% bias for trend confirmation
            return "uptrend"
        elif downtrend_weight > uptrend_weight * 1.2:
            return "downtrend"
        else:
            return "mixed"

    def _calculate_overall_consistency(self, timeframe_results: Dict) -> float:
        """Calculate trend consistency across timeframes"""
        if not timeframe_results:
            return 0.0

        directions = [result.trend_direction for result in timeframe_results.values()]
        confidences = [result.confidence for result in timeframe_results.values()]

        # Count direction agreement
        direction_counts = {}
        weighted_counts = {}

        for direction, confidence in zip(directions, confidences):
            direction_counts[direction] = direction_counts.get(direction, 0) + 1
            weighted_counts[direction] = weighted_counts.get(direction, 0) + confidence

        if not weighted_counts:
            return 0.0

        # Calculate consistency as the dominance of the most common direction
        max_weighted_count = max(weighted_counts.values())
        total_weighted = sum(weighted_counts.values())

        return max_weighted_count / total_weighted if total_weighted > 0 else 0.0

    def _calculate_complexity_score(self, timeframe_results: Dict) -> float:
        """Calculate market complexity score"""
        if not timeframe_results:
            return 0.0

        # Complexity based on variance in trend directions and strengths
        strengths = [
            abs(result.trend_strength) for result in timeframe_results.values()
        ]
        volatilities = [result.volatility for result in timeframe_results.values()]

        # Higher variance = higher complexity
        strength_variance = np.var(strengths) if strengths else 0
        volatility_mean = np.mean(volatilities) if volatilities else 0

        # Normalize to 0-1 range
        complexity = min((strength_variance * 10 + volatility_mean * 2), 1.0)
        return float(complexity)

    def _detect_market_phases(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """Detect and classify market phases"""
        phases = []

        if len(df) < 10:
            return phases

        # Simple phase detection based on price movements
        prices = df["close"].values
        dates = df["date"].tolist()

        # Look for significant price movements
        for i in range(10, len(prices)):
            window_prices = prices[i - 10 : i]
            current_price = prices[i]

            # Check for phase transitions
            trend_change = (current_price - window_prices[0]) / window_prices[0]

            if abs(trend_change) > 0.05:  # 5% threshold for phase detection
                phase_type = "upward_phase" if trend_change > 0 else "downward_phase"
                phases.append(
                    {
                        "start_date": dates[i - 10].strftime("%Y-%m-%d"),
                        "end_date": dates[i].strftime("%Y-%m-%d"),
                        "phase_type": phase_type,
                        "price_change": trend_change,
                    }
                )

        return phases[-5:] if len(phases) > 5 else phases  # Return last 5 phases

    def _generate_overall_assessment(
        self, dominant_trend: str, trend_consistency: float, complexity_score: float
    ) -> str:
        """Generate overall market assessment"""

        if dominant_trend == "insufficient_data":
            return "數據不足，無法進行有效分析"

        consistency_desc = (
            "高度一致"
            if trend_consistency > 0.8
            else "中度一致"
            if trend_consistency > 0.5
            else "分歧明顯"
        )

        complexity_desc = (
            "複雜"
            if complexity_score > 0.6
            else "中等"
            if complexity_score > 0.3
            else "簡單"
        )

        trend_desc = {
            "uptrend": "上升趨勢",
            "downtrend": "下降趨勢",
            "sideways": "橫盤整理",
            "mixed": "混合趨勢",
        }.get(dominant_trend, "未知趨勢")

        return f"市場呈現{trend_desc}，多時間框架{consistency_desc}，市場結構{complexity_desc}"

    def _perform_llm_optimization(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Perform LLM optimization analysis
        執行LLM優化分析，避免固定時間框架滯後問題
        """

        # 1. Multi-timeframe sliding window analysis
        sliding_trends = self._analyze_sliding_windows(df)

        # 2. Market phase detection
        market_phase = self._detect_market_phase(sliding_trends)

        # 3. Trend consistency analysis
        trend_consistency = self._calculate_trend_consistency(sliding_trends)

        # 4. Reversal probability calculation
        reversal_probability = self._calculate_reversal_probability(df, sliding_trends)

        # 5. Momentum analysis
        momentum_status = self._analyze_momentum(df)

        # 6. Risk level assessment
        risk_level = self._assess_risk_level(
            trend_consistency, reversal_probability, momentum_status
        )

        return {
            "market_phase": market_phase,
            "trend_consistency": trend_consistency,
            "reversal_probability": reversal_probability,
            "momentum_status": momentum_status,
            "risk_level": risk_level,
            "sliding_trends": sliding_trends,
        }

    def _analyze_sliding_windows(self, df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
        """Analyze trends using sliding windows to avoid lag"""
        results = {}

        for window in self.sliding_windows:
            if len(df) < window + 5:
                continue

            recent_data = df.tail(window)

            # Calculate trend using linear regression
            x = np.arange(len(recent_data))
            y = recent_data["close"].values

            if len(y) > 1:
                slope, _ = np.polyfit(x, y, 1)
                correlation = np.corrcoef(x, y)[0, 1] if len(y) > 1 else 0

                # Normalize slope by price for comparison
                normalized_slope = slope / recent_data["close"].mean()

                results[f"window_{window}"] = {
                    "slope": normalized_slope,
                    "strength": abs(correlation),
                    "direction": "up" if slope > 0 else "down",
                }

        return results

    def _detect_market_phase(self, sliding_trends: Dict[str, Dict[str, float]]) -> str:
        """Detect current market phase based on multi-timeframe analysis"""
        up_count = 0
        down_count = 0
        total_strength = 0

        for window_data in sliding_trends.values():
            direction = window_data["direction"]
            strength = window_data["strength"]

            if direction == "up":
                up_count += strength
            else:
                down_count += strength

            total_strength += strength

        if total_strength == 0:
            return "consolidation"

        up_ratio = up_count / total_strength
        down_ratio = down_count / total_strength

        # Clear directional bias required
        if up_ratio > 0.65:
            return "uptrend"
        elif down_ratio > 0.65:
            return "downtrend"
        else:
            return "consolidation"

    def _calculate_trend_consistency(
        self, sliding_trends: Dict[str, Dict[str, float]]
    ) -> float:
        """Calculate trend consistency across timeframes"""
        if not sliding_trends:
            return 0.0

        directions = [data["direction"] for data in sliding_trends.values()]
        strengths = [data["strength"] for data in sliding_trends.values()]

        # Calculate weighted consistency
        up_strength = sum(s for d, s in zip(directions, strengths) if d == "up")
        down_strength = sum(s for d, s in zip(directions, strengths) if d == "down")
        total_strength = sum(strengths)

        if total_strength == 0:
            return 0.0

        # Consistency is the dominance of the stronger direction
        consistency = max(up_strength, down_strength) / total_strength
        return min(consistency, 1.0)

    def _calculate_reversal_probability(
        self, df: pd.DataFrame, sliding_trends: Dict[str, Dict[str, float]]
    ) -> float:
        """Calculate probability of trend reversal"""

        # 1. Price structure analysis
        recent_prices = df.tail(20)["close"]
        price_volatility = recent_prices.std() / recent_prices.mean()

        # 2. Trend divergence across timeframes
        short_term_trend = sliding_trends.get("window_5", {}).get(
            "direction", "neutral"
        )
        long_term_trend = sliding_trends.get("window_20", {}).get(
            "direction", "neutral"
        )

        divergence_score = 0.3 if short_term_trend != long_term_trend else 0.0

        # 3. Trend strength deterioration
        strengths = [data["strength"] for data in sliding_trends.values()]
        avg_strength = np.mean(strengths) if strengths else 0
        strength_factor = max(0, 0.8 - avg_strength)  # Higher reversal if weak trends

        # 4. Volatility factor
        volatility_factor = min(
            price_volatility * 2, 0.4
        )  # High volatility = potential reversal

        # Combine factors
        reversal_prob = divergence_score + strength_factor + volatility_factor
        return min(reversal_prob, 1.0)

    def _analyze_momentum(self, df: pd.DataFrame) -> str:
        """Analyze current momentum status"""
        if len(df) < self.momentum_period + 5:
            return "neutral"

        recent_data = df.tail(self.momentum_period)

        # Calculate price momentum
        price_change = (
            recent_data["close"].iloc[-1] - recent_data["close"].iloc[0]
        ) / recent_data["close"].iloc[0]

        # Calculate volume momentum if available
        volume_trend = 0
        if "volume" in recent_data.columns:
            volume_change = (
                recent_data["volume"].iloc[-5:].mean()
                - recent_data["volume"].iloc[:5].mean()
            ) / recent_data["volume"].iloc[:5].mean()
            volume_trend = volume_change

        # Combine price and volume momentum
        momentum_score = price_change + (volume_trend * 0.3)

        if momentum_score > 0.02:
            return "bullish"
        elif momentum_score < -0.02:
            return "bearish"
        else:
            return "neutral"

    def _assess_risk_level(
        self,
        trend_consistency: float,
        reversal_probability: float,
        momentum_status: str,
    ) -> str:
        """Assess current risk level for trading decisions"""

        # High consistency = lower risk
        consistency_risk = 1 - trend_consistency

        # High reversal probability = higher risk
        reversal_risk = reversal_probability

        # Momentum alignment risk
        momentum_risk = 0.3 if momentum_status == "neutral" else 0.0

        total_risk = (consistency_risk + reversal_risk + momentum_risk) / 3

        if total_risk < 0.3:
            return "low"
        elif total_risk < 0.6:
            return "medium"
        else:
            return "high"

    def _generate_decision_context(
        self,
        original_result: TrendAnalysisResult,
        llm_features: Dict[str, Any],
        current_price: float,
    ) -> Dict[str, List[str]]:
        """Generate decision context for LLM"""

        observations = []
        recommendations = []

        # Trend consistency observations
        if llm_features["trend_consistency"] > 0.8:
            observations.append("多時間框架趨勢高度一致，方向明確")
        elif llm_features["trend_consistency"] < 0.4:
            observations.append("不同時間框架趨勢分歧，市場方向不明")

        # Reversal signal observations
        if llm_features["reversal_probability"] > 0.5:
            signal_strength = (
                "強烈" if llm_features["reversal_probability"] > 0.7 else "明顯"
            )
            observations.append(f"檢測到{signal_strength}轉折信號，市場可能正在變化")

        # Momentum observations
        if llm_features["momentum_status"] != "neutral":
            momentum_desc = (
                "加速" if llm_features["trend_consistency"] > 0.7 else "減弱"
            )
            observations.append(
                f"動量呈{llm_features['momentum_status']}態勢，趨勢{momentum_desc}"
            )

        # Generate recommendations
        if llm_features["trend_consistency"] > 0.7:
            recommendations.append("趨勢方向明確，可考慮順勢操作")

        if llm_features["momentum_status"] == "bullish":
            recommendations.append("上漲動量良好，可關注做多機會")
        elif llm_features["momentum_status"] == "bearish":
            recommendations.append("下跌動量明顯，需謹慎防範風險")

        if llm_features["reversal_probability"] > 0.6:
            recommendations.append("轉折信號強烈，建議等待趨勢確認後操作")

        if llm_features["risk_level"] == "high":
            recommendations.append("當前風險水平較高，建議降低倉位或暫停交易")

        return {"observations": observations, "recommendations": recommendations}


def main():
    """Test Enhanced Trend Analyzer with integration example"""
    analyzer = EnhancedTrendAnalyzer()

    print("🚀 Enhanced Trend Analyzer - LLM Integration Test")
    print("=" * 60)

    # Test with latest data
    result = analyzer.analyze_with_llm_optimization("2330.TW")

    print(f"📅 分析日期: {result.analysis_date}")
    print(f"💰 當前價格: ${result.price:.0f}")
    print(f"📊 市場階段: {result.market_phase}")
    print(f"🎯 趨勢一致性: {result.trend_consistency:.2f}")
    print(f"🔄 轉折概率: {result.reversal_probability:.2f}")
    print(f"📈 動量狀態: {result.momentum_status}")
    print(f"⚠️  風險水平: {result.risk_level}")

    print(f"\n🔍 關鍵觀察:")
    for obs in result.key_observations:
        print(f"  • {obs}")

    print(f"\n💡 LLM建議:")
    for rec in result.llm_recommendations:
        print(f"  • {rec}")

    print(f"\n📊 原始分析對比:")
    print(f"  - 主導趨勢: {result.original_result.dominant_trend}")
    print(f"  - 原始一致性: {result.original_result.trend_consistency:.3f}")
    print(f"  - 複雜度分數: {result.original_result.complexity_score:.3f}")

    print(f"\n✅ 系統整合完成!")
    print(f"📈 Enhanced Trend Analyzer 可以直接替換原有 AdvancedTrendAnalyzer")
    print(f"🤖 提供LLM決策優化功能，同時保持向後兼容")


if __name__ == "__main__":
    main()
